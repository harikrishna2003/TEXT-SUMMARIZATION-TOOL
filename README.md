
# Abstractive Text Summarization Using BART and Streamlit

This project focuses on building an **abstractive text summarization** system using state-of-the-art natural language processing (NLP) techniques, specifically leveraging the **BART (Bidirectional and Auto-Regressive Transformers)** model architecture provided by Hugging Face’s Transformers library. The primary goal is to train and deploy a model that can read long documents and generate concise, human-like summaries. To make the project accessible and user-friendly, a **Streamlit web application** was developed, enabling users to interact with the summarization model through a clean graphical interface.

## What Was Done

The project implements a pipeline for training a text summarization model using the **`facebook/bart-base`** pre-trained model on a subset of the **XSum dataset**, a popular benchmark dataset for summarization. The model was fine-tuned on approximately 10,000 training samples and evaluated on a held-out test split (10% of the dataset). Key steps in this project include:

1. **Dataset Preparation**: The `xsum` dataset was loaded using Hugging Face's `datasets` library and then split into training and testing sets.
2. **Model Initialization**: The `facebook/bart-base` pre-trained model and corresponding tokenizer were loaded. The model was then moved to the appropriate device (GPU if available).
3. **Data Preprocessing**: A preprocessing function was defined to tokenize both the document (input text) and the summary (target text). Texts were padded and truncated to specific maximum lengths to fit model constraints.
4. **Training Configuration**: Hugging Face’s `Trainer` API and `TrainingArguments` were used to configure and manage the training process. The model was fine-tuned with a batch size of 2 over 3 epochs with logging and evaluation strategies included.
5. **Model Evaluation**: The model was evaluated using ROUGE metrics after training to measure the quality of the generated summaries.
6. **Streamlit App Integration**: A simple but effective Streamlit interface was developed to allow real-time summarization. Users can input custom text, and the app displays the generated summary after processing the input through the fine-tuned BART model.

## Tools and Technologies Used

This project utilizes a combination of industry-standard tools and libraries from the deep learning and NLP ecosystem:

- **Programming Language**: Python
- **Model Architecture**: BART (`facebook/bart-base`)
- **Datasets Library**: Hugging Face `datasets` for downloading and managing the `xsum` dataset
- **Transformers Library**: Hugging Face `transformers` for pre-trained models and tokenizers
- **Training Utilities**: Hugging Face `Trainer`, `TrainingArguments`, and `DataCollatorForSeq2Seq`
- **Evaluation Metrics**: ROUGE via `evaluate` package
- **Web App Framework**: Streamlit for building the user interface
- **Hardware Acceleration**: PyTorch with CUDA support for GPU acceleration

## Process and Workflow

1. **Dataset Selection**: The `xsum` dataset was chosen for its suitability in abstractive summarization tasks. The dataset provides a variety of news articles and human-written summaries, ideal for training a generative model.
   
2. **Tokenization and Preprocessing**: Documents and summaries were tokenized using `BartTokenizer`. Inputs were padded to a maximum length of 256 tokens, and outputs (summaries) to 64 tokens.

3. **Model Training**: The `Trainer` class streamlined the training process. Training arguments were specified to determine the number of epochs, batch size, logging strategy, and evaluation steps. A `DataCollatorForSeq2Seq` was used to handle dynamic padding during training.

4. **Model Saving and Deployment**: Once trained, the model was saved and loaded in the Streamlit app for inference. The Streamlit app offers an input box for user text and displays a concise summary generated by the fine-tuned model.

5. **Evaluation**: ROUGE scores were computed on the test set to evaluate the performance. The model achieved meaningful ROUGE scores, indicating that it learned to produce coherent and relevant summaries.

## In Conclusion,
   this project successfully demonstrates how powerful transformer-based architectures like BART can be used for abstractive text summarization. By combining efficient training techniques with Hugging Face’s rich ecosystem and deploying it using Streamlit, the project bridges the gap between advanced machine learning and user-centric applications. The final product is a responsive and easy-to-use web app that can summarize any input text, making it suitable for educational tools, research assistants, or even content summarization in real-world business applications.
